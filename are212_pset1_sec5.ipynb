{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Regression in `python`                                   :jupyter:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem set 1, section 5\n",
    "##### Description: Adaptation of code in weighted_regression.ipynb \n",
    "##### Group: Bobing, Cassandra, Max, Prema, Rajdev, Yazen \n",
    "\n",
    "#### Goal: Extend the code to actually estimate beta in the case where k=3 and y=Xbeta+u with ET'u=0 and y is a Nxk matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that $T$ and $u$ are &ldquo;independent&rdquo; (or at least\n",
    "orthogonal) variables means that if we want to compute a\n",
    "&ldquo;classical&rdquo; regression we&rsquo;d do it something like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define independent random variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "k = 3 # Number of observables in T\n",
    "\n",
    "mu = [0]*k\n",
    "Sigma=[[1,0.5,0],\n",
    "       [0.5,2,0],\n",
    "       [0,0,3]]\n",
    "\n",
    "T = multivariate_normal(mu,Sigma)\n",
    "\n",
    "u = multivariate_normal(cov=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define `X`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that $X$ can depend on $T$ and $u$.  This dependence needn&rsquo;t be\n",
    "linear!  For example, suppose $X=T^3D + u$, where $D$ is an\n",
    "$\\ell\\times k$ matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Construct Sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct a sample of observables $(y,X,T)$ we just use the regression equation,\n",
    "      plus an assumption about the value of $\\beta$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = [1/2,1]\n",
    "\n",
    "D = np.random.random(size=(3,2)) # Generate random 3x2 matrix\n",
    "\n",
    "N=1000 # Sample size\n",
    "\n",
    "# Now: Transform rvs into a sample\n",
    "T = T.rvs(N)\n",
    "\n",
    "u = u.rvs(N) # Replace u with a sample\n",
    "\n",
    "X = (T**3)@D  # Note use of ** operator for exponentiation\n",
    "\n",
    "y = X@beta + u # Note use of @ operator for matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Turn to estimation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we now have data on *realizations* $(y,X,T)$.  Now forget\n",
    "     that we know $\\beta$ and let&rsquo;s estimate it, using weighted least\n",
    "     squares.  As a numerical matter it&rsquo;s better to avoid explicitly\n",
    "     inverting the $(T^T X)$ matrix; instead we can solve the &ldquo;normal&rdquo;\n",
    "     equations\n",
    "\n",
    "\\begin{align*}\n",
    "   T'y &= T' X b + T' u\\\\\n",
    "   \\mbox{E}(T'u) = 0\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numerical solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the classical case we were trying to solve a linear system that\n",
    " took the form $Ab=0$, with $A$ a square matrix.  In the present case\n",
    " we&rsquo;re also trying to solve a linear system, but with a matrix $A$\n",
    " that may have more rows than columns.  Provided the rows are linearly\n",
    " independent, this implies that we have an **overidentified** system of\n",
    " equations.  We&rsquo;ll return to the implications of this later, but for\n",
    " now this also calls for a different numerical approach, using\n",
    " `np.linalg.lstsq` instead of `np.linalg.solve`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.50059338 0.99973073]\n",
      "[[ 2.84873063e-06 -6.00942158e-07]\n",
      " [-6.00942158e-07  1.32034333e-06]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-c6e3be43073d>:3: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  b = np.linalg.lstsq(T.T@X,T.T@y)[0] # lstsqs returns several results\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import inv, sqrtm\n",
    "\n",
    "b = np.linalg.lstsq(T.T@X,T.T@y)[0] # lstsqs returns several results\n",
    "\n",
    "e = y - X@b\n",
    "\n",
    "print(b)\n",
    "\n",
    "TXplus = np.linalg.pinv(T.T@X) # Moore-Penrose pseudo-inverse\n",
    "\n",
    "# Covariance matrix of b\n",
    "vb = e.var()*TXplus@T.T@T@TXplus.T  # u is known to be homoskedastic\n",
    "\n",
    "print(vb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 1, Section 5, Question 3: Estimate the code to actually estimate beta in the case with k=3? \n",
    "###### y = (X * beta) + u with ET'u=0\n",
    "###### Given that errors are likely correlated in simultaneous equations, we want to use a matrix of weights that capture this correlation of errors and then use the weighted-least squares method.\n",
    "\n",
    "From Greene, we know that the completeness of the system requires that the number of equations equal the number of endogeneous variables in a simultaneous equation system.\n",
    "\n",
    "The weights will be inversely proportional to the error variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_a=((e*e)/N)\n",
    "W_b=(X.T@X)/N\n",
    "#W= W_a@W_b\n",
    "#print(W_a)\n",
    "len(W_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covariance matrix of b = vb \n",
    "# residuals = e\n",
    "\n",
    "# variance-covariance matrix of error term = (X'X)^{-1}*(residual sum of squares/degrees of freedom)\n",
    "df = n-k-1\n",
    "\n",
    "# reciprocal of var-covar matrix of error term = Weighted matrix \n",
    "\n",
    "W_a=((e*e)/N)\n",
    "W_b = \n",
    "W= W_a@W_b\n",
    "#print(W)\n",
    "\n",
    "# Then we just estimate beta as: (X'WX)^{-1}X'WY\n",
    "b_k3 = np.linalg.pinv(T.T@X)@T.T@y \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
