{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Regression in `python`                                   :jupyter:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem set 1, section 5\n",
    "##### Description: Adaptation of code in weighted_regression.ipynb \n",
    "##### Group: Bobing, Cassandra, Max, Prema, Rajdev, Yazen \n",
    "\n",
    "#### Goal: Extend the code to actually estimate beta in the case where k=3 and y=Xbeta+u with ET'u=0 and y is a Nxk matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that $T$ and $u$ are &ldquo;independent&rdquo; (or at least\n",
    "orthogonal) variables means that if we want to compute a\n",
    "&ldquo;classical&rdquo; regression we&rsquo;d do it something like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define independent random variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "k = 3 # Number of observables in T\n",
    "\n",
    "mu = [0]*k\n",
    "\n",
    "## CHANGE 1: MAKE SIGMA A DIAGONAL MATRIX \n",
    "Sigma=[[1,0,0],\n",
    "       [0,2,0],\n",
    "       [0,0,3]]\n",
    "\n",
    "T = multivariate_normal(mu,Sigma)\n",
    "\n",
    "# CHANGE 2: CHANGE U \n",
    "u = multivariate_normal(mu,Sigma)\n",
    "# u = multivariate_normal(cov=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define `X`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that $X$ can depend on $T$ and $u$.  This dependence needn&rsquo;t be\n",
    "linear!  For example, suppose $X=T^3D + u$, where $D$ is an\n",
    "$\\ell\\times k$ matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Construct Sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct a sample of observables $(y,X,T)$ we just use the regression equation,\n",
    "      plus an assumption about the value of $\\beta$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE 3: MAKE BETA 3X3 \n",
    "beta = [[1/2,1,2],\n",
    "        [2,3,4],\n",
    "        [1,0,6]]\n",
    "\n",
    "# CHANGE 4: MAKE D 3X3\n",
    "D = np.random.random(size=(3,3)) # Generate random 3x2 matrix\n",
    "\n",
    "N=1000 # Sample size\n",
    "\n",
    "# Now: Transform rvs into a sample\n",
    "T = T.rvs(N)\n",
    "\n",
    "u = u.rvs(N) # Replace u with a sample\n",
    "\n",
    "X = (T**3)@D  # Note use of ** operator for exponentiation\n",
    "\n",
    "y = X@beta + u # Note use of @ operator for matrix multiplication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Turn to estimation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we now have data on *realizations* $(y,X,T)$.  Now forget\n",
    "     that we know $\\beta$ and let&rsquo;s estimate it, using weighted least\n",
    "     squares.  As a numerical matter it&rsquo;s better to avoid explicitly\n",
    "     inverting the $(T^T X)$ matrix; instead we can solve the &ldquo;normal&rdquo;\n",
    "     equations\n",
    "\n",
    "\\begin{align*}\n",
    "   T'y &= T' X b + T' u\\\\\n",
    "   \\mbox{E}(T'u) = 0\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numerical solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the classical case we were trying to solve a linear system that\n",
    " took the form $Ab=0$, with $A$ a square matrix.  In the present case\n",
    " we&rsquo;re also trying to solve a linear system, but with a matrix $A$\n",
    " that may have more rows than columns.  Provided the rows are linearly\n",
    " independent, this implies that we have an **overidentified** system of\n",
    " equations.  We&rsquo;ll return to the implications of this later, but for\n",
    " now this also calls for a different numerical approach, using\n",
    " `np.linalg.lstsq` instead of `np.linalg.solve`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.50240756  0.98104413  1.9787112 ]\n",
      " [ 1.98411949  3.05918599  4.03856139]\n",
      " [ 0.98225217 -0.04598303  5.98161753]]\n",
      "[[ 0.00057104 -0.00169626  0.00112185]\n",
      " [-0.00169626  0.00529417 -0.00379543]\n",
      " [ 0.00112185 -0.00379543  0.00350825]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import inv, sqrtm\n",
    "\n",
    "b = np.linalg.lstsq(T.T@X,T.T@y, rcond=None)[0] # lstsqs returns several results\n",
    "\n",
    "e = y - X@b\n",
    "\n",
    "print(b)\n",
    "\n",
    "TXplus = np.linalg.pinv(T.T@X) # Moore-Penrose pseudo-inverse\n",
    "\n",
    "# Covariance matrix of b\n",
    "vb = e.var()*TXplus@T.T@T@TXplus.T  # u is known to be homoskedastic\n",
    "\n",
    "print(vb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
